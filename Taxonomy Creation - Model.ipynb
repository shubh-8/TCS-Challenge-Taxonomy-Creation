{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from wordcloud import WordCloud\n",
    "import re\n",
    "import os\n",
    "import gensim\n",
    "from sqlalchemy import create_engine \n",
    "import datetime as dt\n",
    "import joblib\n",
    "\n",
    "#import nltk\n",
    "#nltk.download('stopwords')\n",
    "#import nltk\n",
    "#nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score,precision_score,recall_score\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taxonomy Creation: Stack ExchangeTag Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_connection(db_file):\n",
    "    \"\"\" create a database connection to the SQLite database\n",
    "        specified by db_file\n",
    "    :param db_file: database file\n",
    "    :return: Connection object or None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        conn = sqlite3.connect(db_file)\n",
    "        return conn\n",
    "    except Error as e:\n",
    "        print(e)\n",
    " \n",
    "    return None\n",
    "\n",
    "def create_table(conn, create_table_sql):\n",
    "    \"\"\" create a table from the create_table_sql statement\n",
    "    :param conn: Connection object\n",
    "    :param create_table_sql: a CREATE TABLE statement\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    try:\n",
    "        c = conn.cursor()\n",
    "        c.execute(create_table_sql)\n",
    "    except Error as e:\n",
    "        print(e)\n",
    "        \n",
    "def checkTableExists(dbcon):\n",
    "    cursr = dbcon.cursor()\n",
    "    str = \"select name from sqlite_master where type='table'\"\n",
    "    table_names = cursr.execute(str)\n",
    "    print(\"Tables in the database:\")\n",
    "    tables =table_names.fetchall() \n",
    "    print(tables[0][0])\n",
    "    return(len(tables))\n",
    "\n",
    "def create_database_table(database, query):\n",
    "    conn = create_connection(database)\n",
    "    if conn is not None:\n",
    "        create_table(conn, query)\n",
    "        checkTableExists(conn)\n",
    "    else:\n",
    "        print(\"Error! cannot create the database connection.\")\n",
    "    conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Taking entries to a dataframe from our saved data\n",
    "write_db = 'Titlemoreweight.db'\n",
    "if os.path.isfile(write_db):\n",
    "    conn_r = create_connection(write_db)\n",
    "    if conn_r is not None:\n",
    "        preprocessed_data = pd.read_sql_query(\"\"\"SELECT question, Tags FROM QuestionsProcessed LIMIT 3000000\"\"\", conn_r)\n",
    "conn_r.commit()\n",
    "conn_r.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2999999, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dynam datagrid bind silverlight dynam datagrid...</td>\n",
       "      <td>c# silverlight data-binding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dynam datagrid bind silverlight dynam datagrid...</td>\n",
       "      <td>c# silverlight data-binding columns</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>java.lang.noclassdeffounderror javax servlet j...</td>\n",
       "      <td>jsp jstl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>java.sql.sqlexcept microsoft odbc driver manag...</td>\n",
       "      <td>java jdbc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>better way updat feed fb php sdk better way up...</td>\n",
       "      <td>facebook api facebook-php-sdk</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  dynam datagrid bind silverlight dynam datagrid...   \n",
       "1  dynam datagrid bind silverlight dynam datagrid...   \n",
       "2  java.lang.noclassdeffounderror javax servlet j...   \n",
       "3  java.sql.sqlexcept microsoft odbc driver manag...   \n",
       "4  better way updat feed fb php sdk better way up...   \n",
       "\n",
       "                                  tags  \n",
       "0          c# silverlight data-binding  \n",
       "1  c# silverlight data-binding columns  \n",
       "2                             jsp jstl  \n",
       "3                            java jdbc  \n",
       "4        facebook api facebook-php-sdk  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(preprocessed_data.shape)\n",
    "preprocessed_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of data points in sample : 2999999\n",
      "number of dimensions : 2\n"
     ]
    }
   ],
   "source": [
    "print(\"number of data points in sample :\", preprocessed_data.shape[0])\n",
    "print(\"number of dimensions :\", preprocessed_data.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ Converting string Tags to multilable output variables __ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(tokenizer = lambda x: x.split(), binary='true')\n",
    "multilabel_y = vectorizer.fit_transform(preprocessed_data['tags'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ Selecting 400 Tags __"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tags_to_choose(n):\n",
    "    t = multilabel_y.sum(axis=0).tolist()[0]\n",
    "    sorted_tags_i = sorted(range(len(t)), key=lambda i: t[i], reverse=True)\n",
    "    multilabel_yn=multilabel_y[:,sorted_tags_i[:n]]\n",
    "    return multilabel_yn\n",
    "\n",
    "def questions_explained_fn(n):\n",
    "    multilabel_yn = tags_to_choose(n)\n",
    "    x= multilabel_yn.sum(axis=1)\n",
    "    return (np.count_nonzero(x==0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of questions that are not covered  346542 out of  2999999\n",
      "With  400 tags we are covering  88.449 % of questions\n"
     ]
    }
   ],
   "source": [
    "no_of_tags = 400\n",
    "total_tags=multilabel_y.shape[1]\n",
    "total_qs=preprocessed_data.shape[0]\n",
    "ques_explained = questions_explained_fn(no_of_tags)\n",
    "# we will be taking 5000 tags\n",
    "multilabel_yx = tags_to_choose(no_of_tags)\n",
    "print(\"number of questions that are not covered \", ques_explained ,\"out of \", total_qs)\n",
    "print(\"With \",no_of_tags,\"tags we are covering \",np.round(((total_qs-ques_explained)/total_qs)*100,3),\"% of questions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_datasize = 2000000\n",
    "# Spliting the dataset into train and test\n",
    "x_train=preprocessed_data.head(train_datasize)\n",
    "x_test=preprocessed_data.tail(preprocessed_data.shape[0] - train_datasize)\n",
    "\n",
    "y_train = multilabel_yx[0:train_datasize,:]\n",
    "y_test = multilabel_yx[train_datasize:preprocessed_data.shape[0],:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data points in train data : 2000000\n",
      "Number of data points in test data : 999999\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of data points in train data :\", y_train.shape[0])\n",
    "print(\"Number of data points in test data :\", y_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Machine Learning Models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting into vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to run this cell : 0:14:26.649355\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "# converting data into BoW Vectorizer\n",
    "count_vect = CountVectorizer(ngram_range=(1,2), max_features=1000)\n",
    "x_train_multilabel = count_vect.fit_transform(x_train['question'])\n",
    "x_test_multilabel = count_vect.transform(x_test['question'])\n",
    "print(\"Time taken to run this cell :\", datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of train data X: (2000000, 1000) Y : (2000000, 400)\n",
      "Dimensions of test data X: (999999, 1000) Y: (999999, 400)\n"
     ]
    }
   ],
   "source": [
    "print(\"Dimensions of train data X:\",x_train_multilabel.shape, \"Y :\",y_train.shape)\n",
    "print(\"Dimensions of test data X:\",x_test_multilabel.shape,\"Y:\",y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Applying  MultinomialNB with OneVsRest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to run this cell : 0:11:09.650740\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "start = datetime.now()\n",
    "\n",
    "classifier_1 = OneVsRestClassifier(MultinomialNB(alpha = 0.001))\n",
    "classifier_1.fit(x_train_multilabel, y_train)\n",
    "print(\"Time taken to run this cell :\", datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.002797002797002797\n",
      "Hamming loss  0.05820874320874321\n",
      "Micro-average quality numbers\n",
      "Precision: 0.0585, Recall: 0.8198, F1-measure: 0.1092\n",
      "Macro-average quality numbers\n",
      "Precision: 0.0479, Recall: 0.8228, F1-measure: 0.0855\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.24      0.62      0.34     69974\n",
      "           1       0.39      0.77      0.51     80038\n",
      "           2       0.36      0.86      0.51     87470\n",
      "           3       0.32      0.84      0.47     63908\n",
      "           4       0.35      0.84      0.50     40510\n",
      "           5       0.29      0.90      0.44     44635\n",
      "           6       0.13      0.72      0.23     27645\n",
      "           7       0.29      0.84      0.43     39786\n",
      "           8       0.22      0.70      0.33     23977\n",
      "           9       0.29      0.93      0.44     45546\n",
      "          10       0.12      0.57      0.19     23442\n",
      "          11       0.15      0.80      0.26     24200\n",
      "          12       0.23      0.72      0.35     23686\n",
      "          13       0.25      0.92      0.39     36451\n",
      "          14       0.18      0.90      0.31     15994\n",
      "          15       0.17      0.68      0.28     23416\n",
      "          16       0.19      0.70      0.30     19962\n",
      "          17       0.30      0.90      0.45     30129\n",
      "          18       0.12      0.84      0.21     19205\n",
      "          19       0.08      0.78      0.15     13471\n",
      "          20       0.08      0.78      0.14     12726\n",
      "          21       0.16      0.92      0.27     18294\n",
      "          22       0.17      0.83      0.28     17502\n",
      "          23       0.16      0.77      0.26     10023\n",
      "          24       0.07      0.83      0.13     11386\n",
      "          25       0.25      0.96      0.40     18750\n",
      "          26       0.14      0.85      0.24     10422\n",
      "          27       0.08      0.88      0.15      9766\n",
      "          28       0.11      0.83      0.19      7869\n",
      "          29       0.10      0.88      0.17      5347\n",
      "          30       0.08      0.91      0.15     10495\n",
      "          31       0.08      0.76      0.14      9515\n",
      "          32       0.07      0.82      0.14      5321\n",
      "          33       0.11      0.93      0.19      4848\n",
      "          34       0.14      0.89      0.24     11698\n",
      "          35       0.09      0.91      0.16      7819\n",
      "          36       0.09      0.67      0.16      7032\n",
      "          37       0.06      0.64      0.12      6789\n",
      "          38       0.07      0.87      0.13      6479\n",
      "          39       0.07      0.89      0.12      8085\n",
      "          40       0.07      0.83      0.13      8040\n",
      "          41       0.05      0.65      0.09      4754\n",
      "          42       0.09      0.74      0.16      7029\n",
      "          43       0.11      0.89      0.20      6978\n",
      "          44       0.06      0.78      0.12      6768\n",
      "          45       0.08      0.88      0.15      5144\n",
      "          46       0.07      0.94      0.13      7510\n",
      "          47       0.05      0.87      0.09      4972\n",
      "          48       0.05      0.82      0.10      5985\n",
      "          49       0.04      0.74      0.07      4116\n",
      "          50       0.05      0.88      0.10      5402\n",
      "          51       0.07      0.92      0.14      7428\n",
      "          52       0.07      0.86      0.13      5722\n",
      "          53       0.06      0.92      0.12      8574\n",
      "          54       0.07      0.93      0.13      7832\n",
      "          55       0.06      0.89      0.12      5835\n",
      "          56       0.07      0.68      0.12      4252\n",
      "          57       0.13      0.93      0.23      7216\n",
      "          58       0.06      0.91      0.11      4292\n",
      "          59       0.06      0.80      0.12      5375\n",
      "          60       0.03      0.67      0.06      2754\n",
      "          61       0.06      0.90      0.11      5377\n",
      "          62       0.06      0.77      0.12      4466\n",
      "          63       0.14      0.89      0.24      6424\n",
      "          64       0.05      0.59      0.09      5309\n",
      "          65       0.11      0.95      0.19      2834\n",
      "          66       0.06      0.77      0.11      6204\n",
      "          67       0.05      0.79      0.09      3051\n",
      "          68       0.04      0.89      0.08      3474\n",
      "          69       0.05      0.89      0.10      3454\n",
      "          70       0.05      0.87      0.09      4378\n",
      "          71       0.06      0.70      0.12      3434\n",
      "          72       0.04      0.91      0.07      3192\n",
      "          73       0.13      0.93      0.23      5206\n",
      "          74       0.08      0.92      0.14      3704\n",
      "          75       0.06      0.87      0.11      3942\n",
      "          76       0.04      0.62      0.08      3709\n",
      "          77       0.05      0.83      0.09      4181\n",
      "          78       0.05      0.51      0.09      6331\n",
      "          79       0.05      0.76      0.09      2120\n",
      "          80       0.05      0.73      0.09      2266\n",
      "          81       0.05      0.87      0.09      4654\n",
      "          82       0.05      0.75      0.09      3355\n",
      "          83       0.04      0.92      0.08      3331\n",
      "          84       0.04      0.67      0.07      3408\n",
      "          85       0.05      0.91      0.10      4487\n",
      "          86       0.04      0.90      0.07      2643\n",
      "          87       0.05      0.94      0.09      4841\n",
      "          88       0.07      0.80      0.13      4740\n",
      "          89       0.04      0.91      0.08      2287\n",
      "          90       0.04      0.90      0.08      2440\n",
      "          91       0.05      0.82      0.09      3911\n",
      "          92       0.04      0.83      0.07      3038\n",
      "          93       0.05      0.89      0.09      3505\n",
      "          94       0.03      0.74      0.05      4733\n",
      "          95       0.04      0.76      0.08      2176\n",
      "          96       0.04      0.72      0.07      4415\n",
      "          97       0.09      0.97      0.16      3109\n",
      "          98       0.05      0.92      0.09      4511\n",
      "          99       0.01      0.70      0.03      2380\n",
      "         100       0.04      0.84      0.08      3112\n",
      "         101       0.10      0.95      0.18      4034\n",
      "         102       0.04      0.87      0.07      4369\n",
      "         103       0.03      0.71      0.05      2143\n",
      "         104       0.03      0.91      0.06      2290\n",
      "         105       0.11      0.74      0.20      5648\n",
      "         106       0.12      0.84      0.21      1907\n",
      "         107       0.03      0.83      0.06      3389\n",
      "         108       0.06      0.86      0.12      2722\n",
      "         109       0.05      0.89      0.10      3832\n",
      "         110       0.11      0.92      0.19      4245\n",
      "         111       0.07      0.94      0.13      3717\n",
      "         112       0.03      0.71      0.07      4487\n",
      "         113       0.03      0.89      0.07      2865\n",
      "         114       0.04      0.83      0.08      2465\n",
      "         115       0.02      0.74      0.04      2635\n",
      "         116       0.05      0.80      0.09      3566\n",
      "         117       0.03      0.66      0.05      3681\n",
      "         118       0.04      0.83      0.08      4519\n",
      "         119       0.02      0.69      0.05      2086\n",
      "         120       0.04      0.89      0.07      2667\n",
      "         121       0.02      0.68      0.05      2090\n",
      "         122       0.04      0.87      0.07      1843\n",
      "         123       0.05      0.85      0.10      2996\n",
      "         124       0.05      0.90      0.10      2878\n",
      "         125       0.04      0.83      0.07      2163\n",
      "         126       0.03      0.79      0.06      1455\n",
      "         127       0.06      0.90      0.11      3410\n",
      "         128       0.08      0.92      0.14      2659\n",
      "         129       0.04      0.78      0.07      2224\n",
      "         130       0.06      0.92      0.12      1622\n",
      "         131       0.04      0.69      0.07      2353\n",
      "         132       0.03      0.84      0.06      2235\n",
      "         133       0.04      0.80      0.07      2139\n",
      "         134       0.08      0.91      0.15      2693\n",
      "         135       0.07      0.94      0.14      3534\n",
      "         136       0.03      0.87      0.06      2302\n",
      "         137       0.04      0.87      0.07      2465\n",
      "         138       0.05      0.94      0.09      2204\n",
      "         139       0.08      0.93      0.14      3013\n",
      "         140       0.04      0.95      0.09      3936\n",
      "         141       0.02      0.73      0.05      2142\n",
      "         142       0.04      0.80      0.07      1365\n",
      "         143       0.01      0.41      0.02      2093\n",
      "         144       0.07      0.95      0.13      2104\n",
      "         145       0.04      0.83      0.07      2386\n",
      "         146       0.03      0.71      0.05      2178\n",
      "         147       0.02      0.80      0.05      3332\n",
      "         148       0.06      0.92      0.10      2698\n",
      "         149       0.05      0.95      0.10      3040\n",
      "         150       0.01      0.64      0.02      1287\n",
      "         151       0.09      0.98      0.17      2694\n",
      "         152       0.03      0.71      0.05      2410\n",
      "         153       0.03      0.67      0.06      3356\n",
      "         154       0.02      0.88      0.05      1533\n",
      "         155       0.04      0.89      0.08      2103\n",
      "         156       0.05      0.80      0.09      2214\n",
      "         157       0.02      0.76      0.05      2336\n",
      "         158       0.03      0.92      0.05      2568\n",
      "         159       0.05      0.87      0.10      1971\n",
      "         160       0.03      0.76      0.06      2239\n",
      "         161       0.04      0.94      0.07      1826\n",
      "         162       0.03      0.84      0.05      1227\n",
      "         163       0.02      0.84      0.04      2417\n",
      "         164       0.03      0.86      0.07      2168\n",
      "         165       0.05      0.85      0.10      2661\n",
      "         166       0.08      0.97      0.15      2836\n",
      "         167       0.03      0.86      0.06      2803\n",
      "         168       0.06      0.92      0.11      2899\n",
      "         169       0.03      0.68      0.06      1746\n",
      "         170       0.03      0.74      0.06      2004\n",
      "         171       0.03      0.85      0.06      2549\n",
      "         172       0.06      0.91      0.12      1924\n",
      "         173       0.04      0.75      0.08      2014\n",
      "         174       0.03      0.94      0.07      3139\n",
      "         175       0.03      0.91      0.06      2033\n",
      "         176       0.03      0.89      0.05      2230\n",
      "         177       0.03      0.93      0.05      2225\n",
      "         178       0.05      0.79      0.09      3069\n",
      "         179       0.02      0.82      0.05      2422\n",
      "         180       0.02      0.63      0.04      1648\n",
      "         181       0.01      0.58      0.02      1054\n",
      "         182       0.03      0.91      0.05      2185\n",
      "         183       0.04      0.77      0.07      2782\n",
      "         184       0.04      0.90      0.07      1809\n",
      "         185       0.06      0.95      0.11      2484\n",
      "         186       0.02      0.76      0.03      1858\n",
      "         187       0.02      0.65      0.04      1922\n",
      "         188       0.02      0.87      0.05      2704\n",
      "         189       0.04      0.83      0.08      1898\n",
      "         190       0.04      0.91      0.07      2223\n",
      "         191       0.01      0.77      0.02      1323\n",
      "         192       0.05      0.87      0.09      1849\n",
      "         193       0.03      0.89      0.07      1803\n",
      "         194       0.01      0.85      0.03      1329\n",
      "         195       0.03      0.92      0.05      2699\n",
      "         196       0.04      0.88      0.08      1694\n",
      "         197       0.03      0.84      0.05      1663\n",
      "         198       0.02      0.82      0.04      1403\n",
      "         199       0.08      0.97      0.14      2272\n",
      "         200       0.02      0.75      0.04      1656\n",
      "         201       0.03      0.93      0.05      3208\n",
      "         202       0.03      0.75      0.05      2014\n",
      "         203       0.03      0.92      0.06      1812\n",
      "         204       0.02      0.87      0.04      1314\n",
      "         205       0.03      0.83      0.06      1944\n",
      "         206       0.03      0.90      0.06      2454\n",
      "         207       0.02      0.84      0.03      1783\n",
      "         208       0.02      0.91      0.05      1533\n",
      "         209       0.02      0.64      0.03      1346\n",
      "         210       0.02      0.87      0.03      1421\n",
      "         211       0.02      0.83      0.04      1355\n",
      "         212       0.06      0.95      0.12      2794\n",
      "         213       0.03      0.95      0.06      2605\n",
      "         214       0.02      0.71      0.04      1425\n",
      "         215       0.02      0.90      0.04      1057\n",
      "         216       0.02      0.66      0.03      1656\n",
      "         217       0.03      0.94      0.06      1469\n",
      "         218       0.03      0.83      0.07      1580\n",
      "         219       0.02      0.88      0.04      1281\n",
      "         220       0.02      0.79      0.04      1360\n",
      "         221       0.04      0.76      0.08      2289\n",
      "         222       0.01      0.71      0.03      1224\n",
      "         223       0.03      0.86      0.06      2094\n",
      "         224       0.06      0.97      0.11      1833\n",
      "         225       0.01      0.77      0.02       927\n",
      "         226       0.04      0.97      0.07      3222\n",
      "         227       0.02      0.65      0.05      1640\n",
      "         228       0.01      0.79      0.03      1613\n",
      "         229       0.01      0.71      0.02      1559\n",
      "         230       0.01      0.63      0.02       961\n",
      "         231       0.02      0.83      0.05      1360\n",
      "         232       0.04      0.88      0.08      2037\n",
      "         233       0.02      0.84      0.04      1067\n",
      "         234       0.04      0.93      0.08      1773\n",
      "         235       0.02      0.95      0.05      1350\n",
      "         236       0.07      0.90      0.13      2240\n",
      "         237       0.02      0.58      0.04      2132\n",
      "         238       0.03      0.87      0.06      2626\n",
      "         239       0.02      0.79      0.04      1650\n",
      "         240       0.03      0.76      0.06      2090\n",
      "         241       0.03      0.67      0.06      2513\n",
      "         242       0.03      0.88      0.05      1861\n",
      "         243       0.02      0.81      0.03      1115\n",
      "         244       0.03      0.74      0.05      1406\n",
      "         245       0.03      0.92      0.07      1932\n",
      "         246       0.01      0.59      0.02       941\n",
      "         247       0.05      0.92      0.09      1095\n",
      "         248       0.03      0.74      0.06      1864\n",
      "         249       0.02      0.92      0.05      1409\n",
      "         250       0.03      0.87      0.06      1212\n",
      "         251       0.02      0.94      0.05      1756\n",
      "         252       0.02      0.88      0.05      1735\n",
      "         253       0.02      0.77      0.04      1376\n",
      "         254       0.05      0.90      0.09      1464\n",
      "         255       0.03      0.88      0.05      1152\n",
      "         256       0.02      0.75      0.04      1512\n",
      "         257       0.02      0.83      0.03      1379\n",
      "         258       0.02      0.74      0.04      1763\n",
      "         259       0.05      0.85      0.10       856\n",
      "         260       0.03      0.90      0.07      1411\n",
      "         261       0.01      0.68      0.02      1247\n",
      "         262       0.01      0.84      0.02      1227\n",
      "         263       0.02      0.87      0.04      1072\n",
      "         264       0.02      0.76      0.04      1520\n",
      "         265       0.01      0.65      0.02      1513\n",
      "         266       0.03      0.92      0.05      1568\n",
      "         267       0.03      0.83      0.06      1684\n",
      "         268       0.01      0.67      0.02      1014\n",
      "         269       0.03      0.94      0.06      1468\n",
      "         270       0.02      0.86      0.05      1179\n",
      "         271       0.02      0.81      0.05      1357\n",
      "         272       0.02      0.91      0.03      1458\n",
      "         273       0.03      0.71      0.05      1562\n",
      "         274       0.01      0.78      0.02       826\n",
      "         275       0.02      0.75      0.04      1484\n",
      "         276       0.05      0.91      0.09      2379\n",
      "         277       0.01      0.65      0.03      1235\n",
      "         278       0.01      0.73      0.02       839\n",
      "         279       0.01      0.85      0.03      1436\n",
      "         280       0.02      0.83      0.03      1630\n",
      "         281       0.01      0.88      0.03       992\n",
      "         282       0.01      0.77      0.02      1409\n",
      "         283       0.02      0.87      0.03       830\n",
      "         284       0.03      0.84      0.06      1168\n",
      "         285       0.02      0.86      0.04      1450\n",
      "         286       0.02      0.80      0.03      1207\n",
      "         287       0.02      0.81      0.03       960\n",
      "         288       0.01      0.82      0.02      1084\n",
      "         289       0.03      0.89      0.06      1680\n",
      "         290       0.02      0.88      0.05      1296\n",
      "         291       0.01      0.87      0.02       811\n",
      "         292       0.02      0.77      0.04      1215\n",
      "         293       0.03      0.70      0.06      1810\n",
      "         294       0.03      0.88      0.05      1429\n",
      "         295       0.02      0.86      0.03      1272\n",
      "         296       0.02      0.81      0.04      1297\n",
      "         297       0.06      0.91      0.12      1852\n",
      "         298       0.01      0.73      0.03      1964\n",
      "         299       0.02      0.85      0.03      1281\n",
      "         300       0.04      0.90      0.08      1812\n",
      "         301       0.01      0.71      0.03      1109\n",
      "         302       0.03      0.81      0.06      1347\n",
      "         303       0.01      0.62      0.02       763\n",
      "         304       0.02      0.89      0.04      1132\n",
      "         305       0.01      0.72      0.03       984\n",
      "         306       0.03      0.84      0.06      1367\n",
      "         307       0.02      0.83      0.04      1359\n",
      "         308       0.04      0.95      0.08      1121\n",
      "         309       0.02      0.84      0.03      1246\n",
      "         310       0.04      0.94      0.08      1260\n",
      "         311       0.05      0.97      0.09      1253\n",
      "         312       0.02      0.87      0.04       926\n",
      "         313       0.01      0.55      0.02      1178\n",
      "         314       0.02      0.82      0.04       920\n",
      "         315       0.01      0.79      0.03      1459\n",
      "         316       0.02      0.70      0.04      1884\n",
      "         317       0.02      0.78      0.03      1178\n",
      "         318       0.03      0.85      0.06      1250\n",
      "         319       0.02      0.89      0.04      1224\n",
      "         320       0.01      0.51      0.02      1008\n",
      "         321       0.03      0.86      0.05      1249\n",
      "         322       0.02      0.79      0.03      1039\n",
      "         323       0.02      0.92      0.04      1111\n",
      "         324       0.01      0.69      0.02      1047\n",
      "         325       0.04      0.95      0.08      1104\n",
      "         326       0.01      0.70      0.01       839\n",
      "         327       0.01      0.91      0.02       578\n",
      "         328       0.02      0.88      0.04      1147\n",
      "         329       0.02      0.79      0.04       823\n",
      "         330       0.02      0.91      0.04      1234\n",
      "         331       0.02      0.61      0.03      1020\n",
      "         332       0.04      0.98      0.08      1221\n",
      "         333       0.02      0.80      0.03      1191\n",
      "         334       0.03      0.89      0.05       815\n",
      "         335       0.02      0.93      0.03       639\n",
      "         336       0.04      0.74      0.07      1552\n",
      "         337       0.03      0.86      0.06       803\n",
      "         338       0.02      0.88      0.04      1188\n",
      "         339       0.02      0.67      0.03      1068\n",
      "         340       0.01      0.78      0.02       956\n",
      "         341       0.01      0.73      0.01       775\n",
      "         342       0.01      0.73      0.03      1002\n",
      "         343       0.02      0.81      0.04       847\n",
      "         344       0.04      0.93      0.07      1266\n",
      "         345       0.01      0.80      0.02      1484\n",
      "         346       0.01      0.80      0.02       798\n",
      "         347       0.03      0.78      0.06      1858\n",
      "         348       0.01      0.73      0.02       814\n",
      "         349       0.01      0.74      0.03       907\n",
      "         350       0.03      0.93      0.06      1236\n",
      "         351       0.00      0.51      0.01       640\n",
      "         352       0.03      0.92      0.06      1153\n",
      "         353       0.00      0.42      0.01       641\n",
      "         354       0.01      0.81      0.02       576\n",
      "         355       0.01      0.86      0.03      1294\n",
      "         356       0.03      0.95      0.05       864\n",
      "         357       0.01      0.80      0.03      1119\n",
      "         358       0.01      0.84      0.03       787\n",
      "         359       0.02      0.81      0.05       823\n",
      "         360       0.02      0.79      0.05      1091\n",
      "         361       0.04      0.97      0.07      1000\n",
      "         362       0.02      0.86      0.03       926\n",
      "         363       0.01      0.80      0.02      1066\n",
      "         364       0.03      0.87      0.07      1255\n",
      "         365       0.02      0.84      0.04      1317\n",
      "         366       0.04      0.85      0.08      1465\n",
      "         367       0.04      0.84      0.07      1349\n",
      "         368       0.02      0.88      0.04       823\n",
      "         369       0.01      0.78      0.03       967\n",
      "         370       0.02      0.82      0.04       658\n",
      "         371       0.01      0.72      0.02       876\n",
      "         372       0.02      0.92      0.04      1398\n",
      "         373       0.02      0.88      0.03      1288\n",
      "         374       0.01      0.67      0.02       554\n",
      "         375       0.02      0.77      0.03       882\n",
      "         376       0.04      0.80      0.08      1169\n",
      "         377       0.04      0.88      0.08      1434\n",
      "         378       0.02      0.78      0.03       910\n",
      "         379       0.01      0.73      0.02       885\n",
      "         380       0.02      0.85      0.04      1150\n",
      "         381       0.01      0.86      0.02       759\n",
      "         382       0.02      0.90      0.04      1229\n",
      "         383       0.02      0.90      0.05       922\n",
      "         384       0.02      0.74      0.04      1006\n",
      "         385       0.01      0.78      0.02       743\n",
      "         386       0.01      0.87      0.03       908\n",
      "         387       0.03      0.92      0.06      1423\n",
      "         388       0.02      0.89      0.05      1008\n",
      "         389       0.02      0.89      0.04       935\n",
      "         390       0.03      0.95      0.06      1064\n",
      "         391       0.03      0.82      0.06      1133\n",
      "         392       0.02      0.83      0.04       984\n",
      "         393       0.02      0.88      0.04       830\n",
      "         394       0.01      0.79      0.02       816\n",
      "         395       0.01      0.80      0.02       821\n",
      "         396       0.02      0.81      0.04      1067\n",
      "         397       0.04      0.90      0.07       765\n",
      "         398       0.02      0.80      0.03      1085\n",
      "         399       0.02      0.63      0.03       963\n",
      "\n",
      "   micro avg       0.06      0.82      0.11   1741468\n",
      "   macro avg       0.05      0.82      0.09   1741468\n",
      "weighted avg       0.15      0.82      0.23   1741468\n",
      " samples avg       0.07      0.73      0.13   1741468\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions_1 = classifier_1.predict(x_test_multilabel)\n",
    "print(\"Accuracy :\",metrics.accuracy_score(y_test, predictions_1))\n",
    "print(\"Hamming loss \",metrics.hamming_loss(y_test,predictions_1))\n",
    "\n",
    "precision = precision_score(y_test, predictions_1, average='micro')\n",
    "recall = recall_score(y_test, predictions_1, average='micro')\n",
    "f1 = f1_score(y_test, predictions_1, average='micro')\n",
    " \n",
    "print(\"Micro-average quality numbers\")\n",
    "print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\n",
    "\n",
    "precision = precision_score(y_test, predictions_1, average='macro')\n",
    "recall = recall_score(y_test, predictions_1, average='macro')\n",
    "f1 = f1_score(y_test, predictions_1, average='macro')\n",
    " \n",
    "print(\"Macro-average quality numbers\")\n",
    "print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\n",
    "\n",
    "print (metrics.classification_report(y_test, predictions_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NB_model.sav']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = 'NB_model.sav'\n",
    "joblib.dump(classifier_1, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the model from disk\n",
    "filename = 'NB_model.sav'\n",
    "classifier_1 = joblib.load(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question  101 : \n",
      " java getconstructor java getconstructor java getconstructor wrote question comment code think easier understand way suggest thank advanc\n",
      "\n",
      "Taxonomy created (Predicted Tags with their importance ) : \n",
      "99.96 \t\t .app\n",
      "\n",
      " True Tags:\n",
      ".app\n",
      "8087cw\n"
     ]
    }
   ],
   "source": [
    "# example : output from the trained model\n",
    "q_no = 101\n",
    "ques_with_imp = dict()\n",
    "tag_names = vectorizer.get_feature_names()\n",
    "\n",
    "predictions = classifier_1.predict(x_test_multilabel[q_no])\n",
    "pred_prob = (classifier_1.predict_proba(x_test_multilabel[q_no])[0])\n",
    "\n",
    "print(\"Question \",q_no,\": \\n\",list(x_test['question'])[q_no])\n",
    "\n",
    "print(\"\\nTaxonomy created (Predicted Tags with their importance ) : \")\n",
    "pred_indx = np.nonzero(predictions.toarray()[0])\n",
    "\n",
    "for i in pred_indx[0]:\n",
    "    if pred_prob[i]*100 > 80:\n",
    "        ques_with_imp[tag_names[i]] = np.round(pred_prob[i]*100,2)\n",
    "ques_with_imp = sorted(ques_with_imp.items(), key=lambda kv: kv[1],reverse=True)\n",
    "\n",
    "for i,j in ques_with_imp:\n",
    "    print(j,\"\\t\\t\",i)\n",
    "    \n",
    "print(\"\\nTrue Tags:\")\n",
    "test_indx = np.nonzero(y_test.toarray()[q_no])\n",
    "for i in test_indx[0]:\n",
    "    print(tag_names[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Applying GridSearchCV for Logistic Regression with OneVsRest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparam tuning on lambda for Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 4 candidates, totalling 8 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of   8 | elapsed: 22.0min remaining:  7.3min\n",
      "[Parallel(n_jobs=-1)]: Done   8 out of   8 | elapsed: 74.9min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=2, error_score='raise-deprecating',\n",
       "                   estimator=OneVsRestClassifier(estimator=LogisticRegression(C=1.0,\n",
       "                                                                              class_weight=None,\n",
       "                                                                              dual=False,\n",
       "                                                                              fit_intercept=True,\n",
       "                                                                              intercept_scaling=1,\n",
       "                                                                              l1_ratio=None,\n",
       "                                                                              max_iter=100,\n",
       "                                                                              multi_class='warn',\n",
       "                                                                              n_jobs=None,\n",
       "                                                                              penalty='l1',\n",
       "                                                                              random_state=None,\n",
       "                                                                              solver='warn',\n",
       "                                                                              tol=0.0001,\n",
       "                                                                              verbose=0,\n",
       "                                                                              warm_start=False),\n",
       "                                                 n_jobs=None),\n",
       "                   iid='warn', n_iter=10, n_jobs=-1,\n",
       "                   param_distributions={'estimator__C': [0.0001, 0.01, 1, 100]},\n",
       "                   pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "                   return_train_score=False, scoring='f1_micro', verbose=2)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "param = {\"estimator__C\": [10**-4, 10**-2, 10**0, 10**2]}\n",
    "n_folds = 2\n",
    "classifier_2 = OneVsRestClassifier(LogisticRegression(penalty='l1'))\n",
    "rndm = RandomizedSearchCV(estimator=classifier_2, param_distributions=param, cv=n_folds, scoring='f1_micro',verbose=2, n_jobs=-1)\n",
    "rndm.fit(x_train_multilabel, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best estimator for the model :\n",
      "  OneVsRestClassifier(estimator=LogisticRegression(C=1, class_weight=None,\n",
      "                                                 dual=False, fit_intercept=True,\n",
      "                                                 intercept_scaling=1,\n",
      "                                                 l1_ratio=None, max_iter=100,\n",
      "                                                 multi_class='warn',\n",
      "                                                 n_jobs=None, penalty='l1',\n",
      "                                                 random_state=None,\n",
      "                                                 solver='warn', tol=0.0001,\n",
      "                                                 verbose=0, warm_start=False),\n",
      "                    n_jobs=None)\n",
      "Best Score for the model :  0.36279099485260147\n"
     ]
    }
   ],
   "source": [
    "print(\"Best estimator for the model :\\n \",rndm.best_estimator_)\n",
    "print(\"Best Score for the model : \",rndm.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LR_model']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#save model\n",
    "joblib.dump(classifier_2, 'LR_model') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the model from disk\n",
    "filename = 'LR_model'\n",
    "classifier_2 = joblib.load(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.1472\n",
      "Hamming loss  0.00436925\n",
      "Micro-average quality numbers\n",
      "Precision: 0.5162, Recall: 0.2363, F1-measure: 0.3242\n",
      "Macro-average quality numbers\n",
      "Precision: 0.2739, Recall: 0.1665, F1-measure: 0.1918\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.35      0.49       820\n",
      "           1       0.49      0.03      0.06      1931\n",
      "           2       0.50      0.12      0.19       544\n",
      "           3       0.63      0.18      0.29       222\n",
      "           4       0.80      0.43      0.56      1311\n",
      "           5       0.84      0.44      0.58      1014\n",
      "           6       0.76      0.32      0.45      1374\n",
      "           7       0.82      0.53      0.65       702\n",
      "           8       0.88      0.55      0.68      1424\n",
      "           9       0.76      0.09      0.16      1037\n",
      "          10       0.61      0.25      0.35       797\n",
      "          11       0.67      0.37      0.47       156\n",
      "          12       0.57      0.44      0.50        36\n",
      "          13       0.73      0.34      0.46       610\n",
      "          14       0.46      0.17      0.25       405\n",
      "          15       0.72      0.18      0.29       144\n",
      "          16       0.55      0.20      0.29       425\n",
      "          17       0.37      0.09      0.14       485\n",
      "          18       0.79      0.69      0.74       269\n",
      "          19       0.84      0.55      0.67       518\n",
      "          20       0.63      0.12      0.20       529\n",
      "          21       0.78      0.53      0.63       294\n",
      "          22       0.79      0.33      0.46       520\n",
      "          23       0.62      0.26      0.36       246\n",
      "          24       0.48      0.15      0.23       312\n",
      "          25       0.44      0.24      0.31       314\n",
      "          26       0.65      0.28      0.39       190\n",
      "          27       0.28      0.07      0.11       342\n",
      "          28       0.35      0.15      0.21        96\n",
      "          29       0.00      0.00      0.00        32\n",
      "          30       0.46      0.41      0.43       747\n",
      "          31       0.50      0.21      0.30        14\n",
      "          32       0.58      0.61      0.59       166\n",
      "          33       0.55      0.22      0.32       171\n",
      "          34       0.58      0.23      0.33       256\n",
      "          35       0.82      0.53      0.64       199\n",
      "          36       0.09      0.02      0.03        60\n",
      "          37       0.23      0.16      0.19       203\n",
      "          38       0.63      0.42      0.51       201\n",
      "          39       0.32      0.22      0.26       208\n",
      "          40       0.12      0.08      0.10        13\n",
      "          41       0.50      0.13      0.21       154\n",
      "          42       0.45      0.29      0.35        69\n",
      "          43       0.10      0.00      0.01       426\n",
      "          44       0.45      0.27      0.34        77\n",
      "          45       0.53      0.29      0.38       223\n",
      "          46       0.23      0.10      0.14       144\n",
      "          47       0.42      0.08      0.13       245\n",
      "          48       0.32      0.12      0.18        91\n",
      "          49       0.57      0.22      0.31       157\n",
      "          50       0.82      0.68      0.74       132\n",
      "          51       0.80      0.59      0.68        41\n",
      "          52       0.57      0.32      0.41       124\n",
      "          53       0.23      0.32      0.27        96\n",
      "          54       0.20      0.12      0.15       128\n",
      "          55       0.48      0.22      0.30        46\n",
      "          56       0.61      0.55      0.58       151\n",
      "          57       0.00      0.00      0.00        80\n",
      "          58       0.28      0.14      0.19        65\n",
      "          59       0.42      0.18      0.25       182\n",
      "          60       0.82      0.72      0.77       148\n",
      "          61       0.34      0.16      0.22       196\n",
      "          62       0.33      0.17      0.23        58\n",
      "          63       0.86      0.28      0.42        43\n",
      "          64       0.20      0.02      0.03       197\n",
      "          65       0.46      0.33      0.38        82\n",
      "          66       0.42      0.16      0.23        50\n",
      "          67       0.61      0.53      0.57       105\n",
      "          68       0.22      0.07      0.11        98\n",
      "          69       0.21      0.10      0.13       238\n",
      "          70       0.00      0.00      0.00        35\n",
      "          71       0.44      0.37      0.40        54\n",
      "          72       0.14      0.04      0.06        25\n",
      "          73       0.33      0.14      0.20        29\n",
      "          74       0.15      0.10      0.12        29\n",
      "          75       0.35      0.20      0.25        40\n",
      "          76       0.76      0.56      0.64       105\n",
      "          77       0.55      0.39      0.46        28\n",
      "          78       0.11      0.04      0.06       202\n",
      "          79       0.50      0.35      0.41        37\n",
      "          80       0.67      0.27      0.38        15\n",
      "          81       0.42      0.27      0.33        52\n",
      "          82       0.26      0.18      0.21        50\n",
      "          83       0.13      0.04      0.06        56\n",
      "          84       0.64      0.50      0.56        54\n",
      "          85       0.48      0.47      0.48        34\n",
      "          86       0.19      0.10      0.13        30\n",
      "          87       0.46      0.21      0.29        29\n",
      "          88       0.59      0.71      0.64        24\n",
      "          89       0.64      0.65      0.64       117\n",
      "          90       0.20      0.09      0.13        66\n",
      "          91       0.41      0.24      0.30        68\n",
      "          92       0.00      0.00      0.00        67\n",
      "          93       0.35      0.21      0.27        28\n",
      "          94       0.36      0.24      0.29        17\n",
      "          95       0.77      0.45      0.57        51\n",
      "          96       0.51      0.36      0.42        53\n",
      "          97       0.00      0.00      0.00        61\n",
      "          98       0.00      0.00      0.00        79\n",
      "          99       0.54      0.39      0.45        18\n",
      "         100       0.00      0.00      0.00        11\n",
      "         101       0.15      0.07      0.09       207\n",
      "         102       0.00      0.00      0.00         6\n",
      "         103       0.00      0.00      0.00        30\n",
      "         104       0.25      0.06      0.09        54\n",
      "         105       0.00      0.00      0.00        39\n",
      "         106       0.35      0.16      0.22        70\n",
      "         107       0.25      0.14      0.18        14\n",
      "         108       0.00      0.00      0.00        66\n",
      "         109       0.52      0.28      0.36        50\n",
      "         110       0.07      0.01      0.02        87\n",
      "         111       0.42      0.25      0.32        51\n",
      "         112       0.33      0.01      0.01       291\n",
      "         113       0.95      0.71      0.81        49\n",
      "         114       0.35      0.17      0.23       110\n",
      "         115       0.25      0.07      0.11        28\n",
      "         116       0.00      0.00      0.00         5\n",
      "         117       0.23      0.11      0.15        56\n",
      "         118       0.08      0.01      0.01       125\n",
      "         119       0.79      0.50      0.61        44\n",
      "         120       0.50      0.12      0.19        42\n",
      "         121       0.37      0.20      0.26        55\n",
      "         122       0.70      0.46      0.55        68\n",
      "         123       0.05      0.04      0.04        82\n",
      "         124       0.00      0.00      0.00         0\n",
      "         125       0.04      0.14      0.06         7\n",
      "         126       0.00      0.00      0.00        18\n",
      "         127       0.45      0.16      0.24        31\n",
      "         128       0.33      0.23      0.27        13\n",
      "         129       0.62      0.52      0.57        50\n",
      "         130       0.17      0.10      0.13        91\n",
      "         131       0.67      0.57      0.62        35\n",
      "         132       0.20      0.19      0.20        26\n",
      "         133       0.00      0.00      0.00        32\n",
      "         134       0.64      0.40      0.49        35\n",
      "         135       0.07      0.03      0.04        37\n",
      "         136       0.00      0.00      0.00        55\n",
      "         137       0.26      0.37      0.31        41\n",
      "         138       0.29      0.13      0.18        15\n",
      "         139       0.28      0.16      0.21        99\n",
      "         140       0.21      0.08      0.12        86\n",
      "         141       0.50      0.26      0.35        53\n",
      "         142       0.40      0.11      0.17        36\n",
      "         143       0.52      0.48      0.50        66\n",
      "         144       0.22      0.09      0.13        64\n",
      "         145       0.17      0.08      0.11        25\n",
      "         146       0.22      0.16      0.19       125\n",
      "         147       0.25      0.33      0.29        15\n",
      "         148       0.04      0.02      0.03        48\n",
      "         149       0.38      0.32      0.35        65\n",
      "         150       0.00      0.00      0.00        11\n",
      "         151       0.31      0.27      0.29        15\n",
      "         152       0.20      0.15      0.17        52\n",
      "         153       0.46      0.33      0.39        18\n",
      "         154       0.12      0.06      0.08        16\n",
      "         155       0.09      0.05      0.06        20\n",
      "         156       0.47      0.21      0.30       121\n",
      "         157       0.46      0.40      0.43       107\n",
      "         158       0.00      0.00      0.00        15\n",
      "         159       0.12      0.03      0.05       105\n",
      "         160       0.39      0.35      0.37        69\n",
      "         161       0.49      0.30      0.37        56\n",
      "         162       0.19      0.06      0.10        47\n",
      "         163       0.17      0.06      0.09       121\n",
      "         164       0.34      0.27      0.30        41\n",
      "         165       0.22      0.01      0.02       229\n",
      "         166       0.07      0.01      0.02        98\n",
      "         167       0.09      0.03      0.05        33\n",
      "         168       0.43      0.07      0.12        44\n",
      "         169       0.26      0.18      0.21        45\n",
      "         170       0.20      0.02      0.04        51\n",
      "         171       0.00      0.00      0.00        18\n",
      "         172       0.55      0.46      0.50        48\n",
      "         173       0.40      0.17      0.24        12\n",
      "         174       0.26      0.16      0.20        62\n",
      "         175       0.76      0.36      0.49        44\n",
      "         176       0.22      0.13      0.17        30\n",
      "         177       0.00      0.00      0.00        30\n",
      "         178       0.00      0.00      0.00         0\n",
      "         179       0.05      1.00      0.09         1\n",
      "         180       0.16      0.07      0.10        40\n",
      "         181       0.21      0.09      0.13        44\n",
      "         182       0.12      0.50      0.20         2\n",
      "         183       0.48      0.39      0.43        75\n",
      "         184       0.07      0.25      0.11         4\n",
      "         185       0.35      0.17      0.23        64\n",
      "         186       0.30      0.25      0.27        12\n",
      "         187       0.50      0.02      0.04        55\n",
      "         188       0.41      0.11      0.17        64\n",
      "         189       0.31      0.21      0.25        96\n",
      "         190       0.20      0.05      0.07        22\n",
      "         191       0.25      0.05      0.09        76\n",
      "         192       0.40      0.51      0.45        45\n",
      "         193       0.78      0.50      0.61        14\n",
      "         194       0.40      0.36      0.38        50\n",
      "         195       0.35      0.40      0.37        20\n",
      "         196       0.10      0.03      0.04        35\n",
      "         197       0.57      0.33      0.42        94\n",
      "         198       0.00      0.00      0.00        14\n",
      "         199       0.00      0.00      0.00        25\n",
      "         200       0.25      0.04      0.06        54\n",
      "         201       0.17      0.09      0.12        22\n",
      "         202       0.13      0.14      0.14        43\n",
      "         203       0.13      0.05      0.07        43\n",
      "         204       0.50      0.37      0.43        62\n",
      "         205       0.00      0.00      0.00         3\n",
      "         206       0.26      0.21      0.23        43\n",
      "         207       0.00      0.00      0.00         7\n",
      "         208       0.12      0.12      0.12         8\n",
      "         209       0.17      0.05      0.07        42\n",
      "         210       0.50      0.50      0.50        10\n",
      "         211       0.38      0.28      0.32        40\n",
      "         212       0.17      0.04      0.07        23\n",
      "         213       0.00      0.00      0.00         6\n",
      "         214       0.66      0.40      0.50        47\n",
      "         215       0.00      0.00      0.00        62\n",
      "         216       0.43      0.17      0.24        77\n",
      "         217       0.11      0.05      0.06        22\n",
      "         218       0.00      0.00      0.00         3\n",
      "         219       0.05      0.04      0.04        28\n",
      "         220       0.05      0.02      0.03        81\n",
      "         221       0.19      0.23      0.21        31\n",
      "         222       0.33      0.06      0.10        34\n",
      "         223       0.52      0.18      0.27        60\n",
      "         224       0.38      0.30      0.33        10\n",
      "         225       0.11      0.20      0.14        10\n",
      "         226       0.38      0.23      0.29        92\n",
      "         227       0.67      0.15      0.25        13\n",
      "         228       0.00      0.00      0.00        13\n",
      "         229       0.33      0.09      0.15        43\n",
      "         230       0.38      0.17      0.24        35\n",
      "         231       0.00      0.00      0.00         4\n",
      "         232       0.30      0.15      0.20        20\n",
      "         233       0.16      0.10      0.12       145\n",
      "         234       0.00      0.00      0.00        55\n",
      "         235       0.00      0.00      0.00         2\n",
      "         236       0.09      0.05      0.07        37\n",
      "         237       0.00      0.00      0.00        90\n",
      "         238       0.14      0.02      0.03        58\n",
      "         239       0.00      0.00      0.00        20\n",
      "         240       0.25      0.05      0.08        61\n",
      "         241       0.00      0.00      0.00        42\n",
      "         242       0.09      0.03      0.05        30\n",
      "         243       0.20      0.14      0.16        66\n",
      "         244       0.69      0.26      0.38        42\n",
      "         245       0.08      0.06      0.07        31\n",
      "         246       0.25      0.50      0.33         6\n",
      "         247       0.25      0.06      0.09        18\n",
      "         248       0.18      0.08      0.11        51\n",
      "         249       0.00      0.00      0.00        17\n",
      "         250       0.15      0.09      0.11        22\n",
      "         251       0.00      0.00      0.00        52\n",
      "         252       0.38      0.21      0.27        29\n",
      "         253       0.00      0.00      0.00        28\n",
      "         254       0.00      0.00      0.00        10\n",
      "         255       0.12      0.20      0.15         5\n",
      "         256       0.00      0.00      0.00         3\n",
      "         257       0.10      0.05      0.07        41\n",
      "         258       0.31      0.13      0.19        30\n",
      "         259       0.12      0.33      0.18         3\n",
      "         260       0.00      0.00      0.00        38\n",
      "         261       0.00      0.00      0.00         1\n",
      "         262       0.00      0.00      0.00        19\n",
      "         263       0.07      0.07      0.07        14\n",
      "         264       0.15      0.11      0.13        37\n",
      "         265       0.00      0.00      0.00         9\n",
      "         266       0.16      0.24      0.20        45\n",
      "         267       0.46      0.39      0.43        33\n",
      "         268       0.33      0.06      0.11        16\n",
      "         269       0.13      0.09      0.10        35\n",
      "         270       0.43      0.27      0.33        11\n",
      "         271       0.00      0.00      0.00        30\n",
      "         272       0.33      0.38      0.35         8\n",
      "         273       0.11      0.10      0.10        21\n",
      "         274       0.09      0.06      0.07       123\n",
      "         275       0.04      0.01      0.02        67\n",
      "         276       0.13      0.10      0.11        20\n",
      "         277       0.00      0.00      0.00        14\n",
      "         278       0.21      0.16      0.18        19\n",
      "         279       0.17      0.25      0.20        12\n",
      "         280       0.17      0.07      0.10        15\n",
      "         281       0.00      0.00      0.00        17\n",
      "         282       0.00      0.00      0.00        41\n",
      "         283       0.46      0.40      0.43        15\n",
      "         284       0.28      0.07      0.11        74\n",
      "         285       0.00      0.00      0.00        38\n",
      "         286       0.08      0.06      0.07        16\n",
      "         287       0.25      0.03      0.06        30\n",
      "         288       0.09      0.04      0.05        28\n",
      "         289       0.17      0.05      0.07        21\n",
      "         290       0.09      0.02      0.04        41\n",
      "         291       0.00      0.00      0.00        12\n",
      "         292       0.44      0.17      0.24        24\n",
      "         293       0.42      0.50      0.45        20\n",
      "         294       0.14      0.13      0.13        23\n",
      "         295       0.09      0.03      0.05        29\n",
      "         296       0.33      0.29      0.31        28\n",
      "         297       0.14      0.02      0.04        42\n",
      "         298       0.17      0.09      0.12        53\n",
      "         299       0.08      0.03      0.04        36\n",
      "         300       0.21      0.07      0.11        41\n",
      "         301       0.41      0.38      0.39        37\n",
      "         302       0.33      0.12      0.17        26\n",
      "         303       0.25      0.36      0.30        11\n",
      "         304       0.07      0.03      0.04        31\n",
      "         305       0.33      0.24      0.28        17\n",
      "         306       0.17      0.11      0.13         9\n",
      "         307       0.00      0.00      0.00         6\n",
      "         308       0.00      0.00      0.00        34\n",
      "         309       0.52      0.30      0.38        43\n",
      "         310       0.12      0.10      0.11        30\n",
      "         311       0.28      0.22      0.25        50\n",
      "         312       0.25      0.08      0.12        24\n",
      "         313       0.13      0.05      0.07        42\n",
      "         314       0.40      0.18      0.25        22\n",
      "         315       0.25      0.03      0.06        58\n",
      "         316       0.00      0.00      0.00        10\n",
      "         317       0.31      0.19      0.24        57\n",
      "         318       0.33      0.10      0.15        10\n",
      "         319       0.00      0.00      0.00        11\n",
      "         320       0.27      0.27      0.27        11\n",
      "         321       1.00      0.12      0.22         8\n",
      "         322       0.00      0.00      0.00        22\n",
      "         323       0.00      0.00      0.00        28\n",
      "         324       0.24      0.10      0.14        50\n",
      "         325       0.20      0.17      0.18        18\n",
      "         326       0.06      0.03      0.04        33\n",
      "         327       0.09      0.18      0.12        17\n",
      "         328       0.27      0.10      0.15        29\n",
      "         329       0.00      0.00      0.00         7\n",
      "         330       0.00      0.00      0.00        10\n",
      "         331       0.25      0.16      0.20        25\n",
      "         332       0.00      0.00      0.00         2\n",
      "         333       0.60      0.55      0.57        11\n",
      "         334       0.00      0.00      0.00        24\n",
      "         335       0.00      0.00      0.00         5\n",
      "         336       0.12      0.03      0.05        33\n",
      "         337       0.18      0.07      0.10        30\n",
      "         338       0.00      0.00      0.00        42\n",
      "         339       0.21      0.12      0.15        26\n",
      "         340       0.40      0.33      0.36        36\n",
      "         341       0.60      0.23      0.33        13\n",
      "         342       0.09      0.09      0.09        11\n",
      "         343       0.25      0.10      0.14        10\n",
      "         344       0.08      0.05      0.06        21\n",
      "         345       0.00      0.00      0.00         0\n",
      "         346       0.00      0.00      0.00         6\n",
      "         347       0.15      0.17      0.16        12\n",
      "         348       0.00      0.00      0.00        13\n",
      "         349       0.40      0.17      0.24        24\n",
      "         350       0.06      0.04      0.05        27\n",
      "         351       0.24      0.09      0.13        43\n",
      "         352       0.00      0.00      0.00        30\n",
      "         353       0.00      0.00      0.00        22\n",
      "         354       0.06      0.03      0.04        31\n",
      "         355       0.35      0.80      0.48        10\n",
      "         356       0.33      0.10      0.15        20\n",
      "         357       0.00      0.00      0.00        20\n",
      "         358       0.48      0.43      0.45        28\n",
      "         359       0.55      0.57      0.56        21\n",
      "         360       0.11      0.04      0.06        25\n",
      "         361       0.10      0.11      0.11        35\n",
      "         362       0.00      0.00      0.00        36\n",
      "         363       0.22      0.24      0.23        17\n",
      "         364       0.20      0.15      0.17        13\n",
      "         365       0.00      0.00      0.00        21\n",
      "         366       1.00      0.06      0.11        18\n",
      "         367       0.23      0.03      0.05        97\n",
      "         368       0.48      0.48      0.48        29\n",
      "         369       0.00      0.00      0.00        12\n",
      "         370       0.27      0.31      0.29        13\n",
      "         371       0.09      0.11      0.10        18\n",
      "         372       0.00      0.00      0.00         6\n",
      "         373       0.50      0.17      0.25         6\n",
      "         374       0.40      0.13      0.20        30\n",
      "         375       0.32      0.26      0.29        27\n",
      "         376       0.00      0.00      0.00        28\n",
      "         377       0.00      0.00      0.00         2\n",
      "         378       0.20      0.25      0.22         4\n",
      "         379       0.00      0.00      0.00        19\n",
      "         380       0.18      0.40      0.25         5\n",
      "         381       0.08      0.06      0.07        18\n",
      "         382       0.31      0.23      0.26        22\n",
      "         383       0.00      0.00      0.00        16\n",
      "         384       0.67      0.46      0.55        13\n",
      "         385       0.22      0.11      0.15        18\n",
      "         386       0.00      0.00      0.00        11\n",
      "         387       0.08      0.01      0.02        88\n",
      "         388       0.00      0.00      0.00        13\n",
      "         389       0.17      0.17      0.17         6\n",
      "         390       0.00      0.00      0.00         6\n",
      "         391       0.00      0.00      0.00        51\n",
      "         392       0.00      0.00      0.00        13\n",
      "         393       0.22      0.05      0.09        37\n",
      "         394       0.00      0.00      0.00         6\n",
      "         395       0.00      0.00      0.00         9\n",
      "         396       0.00      0.00      0.00        13\n",
      "         397       0.00      0.00      0.00         6\n",
      "         398       0.12      0.07      0.09        29\n",
      "         399       0.25      0.03      0.05        33\n",
      "\n",
      "   micro avg       0.52      0.24      0.32     35481\n",
      "   macro avg       0.27      0.17      0.19     35481\n",
      "weighted avg       0.49      0.24      0.30     35481\n",
      " samples avg       0.29      0.23      0.23     35481\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classifier_2.fit(x_train_multilabel, y_train)\n",
    "\n",
    "predictions_2 = classifier_2.predict(x_test_multilabel)\n",
    "print(\"Accuracy :\",metrics.accuracy_score(y_test, predictions_2))\n",
    "print(\"Hamming loss \",metrics.hamming_loss(y_test,predictions_2))\n",
    "\n",
    "\n",
    "precision = precision_score(y_test, predictions_2, average='micro')\n",
    "recall = recall_score(y_test, predictions_2, average='micro')\n",
    "f1 = f1_score(y_test, predictions_2, average='micro')\n",
    " \n",
    "print(\"Micro-average quality numbers\")\n",
    "print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\n",
    "\n",
    "precision = precision_score(y_test, predictions_2, average='macro')\n",
    "recall = recall_score(y_test, predictions_2, average='macro')\n",
    "f1 = f1_score(y_test, predictions_2, average='macro')\n",
    " \n",
    "print(\"Macro-average quality numbers\")\n",
    "print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\n",
    "\n",
    "print (metrics.classification_report(y_test, predictions_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question  0 : \n",
      " abort socket oper window phone abort socket oper window phone abort socket oper window phone use pseudo-synchron socket window phone applic socket code base sampl http msdn.microsoft.com en-us librari hh202858 vs.92 .aspx server send pattern somewhat unpredict start fixed-s header contain length rest messag first read header read specifi number byte socket sinc need send messag server well attempt duplex socket thread receiv anoth thread send caus lot problem loop like code work fine major time strang thing happen messag null nbecaus timeout receiv method see link sampl use manualresetev receiv request socket never actual cancel even though method return request wait around somewher data avail socket chomp header event handler noth data receiv sinc method return variabl method never use data basic disappear read request expect return header skip read byte header idea long messag like abl cancel outstand request socket time use anonym method like sampl sinc simplifi everyth prevent write state transfer code thus unhook event handler think though even use method event handler unhook asynchron oper done callback method would still call test understand right solut see hack togeth static byte array ie static byte header null read header otherwis read messag seem like realli ineleg solut prone race condit better way thank\n",
      "\n",
      "Taxonomy created (Predicted Tags with their importance ) : \n",
      "99.89 \t\t 502-error\n",
      "80.64 \t\t acceleratorkey\n",
      "71.32 \t\t .bash-profile\n",
      "\n",
      " True Tags:\n",
      "502-error\n",
      "acceleratorkey\n"
     ]
    }
   ],
   "source": [
    "# example : output from the trained model\n",
    "q_no = 0\n",
    "ques_with_imp = dict()\n",
    "tag_names = vectorizer.get_feature_names()\n",
    "\n",
    "predictions = classifier_2.predict(x_test_multilabel[q_no])\n",
    "pred_prob = (classifier_2.predict_proba(x_test_multilabel[q_no])[0])\n",
    "\n",
    "print(\"Question \",q_no,\": \\n\",list(x_test['question'])[q_no])\n",
    "\n",
    "print(\"\\nTaxonomy created (Predicted Tags with their importance ) : \")\n",
    "pred_indx = np.nonzero(predictions.toarray()[0])\n",
    "\n",
    "for i in pred_indx[0]:\n",
    "    ques_with_imp[tag_names[i]] = np.round(pred_prob[i]*100,2)\n",
    "ques_with_imp = sorted(ques_with_imp.items(), key=lambda kv: kv[1],reverse=True)\n",
    "\n",
    "for i,j in ques_with_imp:\n",
    "    print(j,\"\\t\\t\",i)\n",
    "    \n",
    "print(\"\\n True Tags:\")\n",
    "test_indx = np.nonzero(y_test.toarray()[q_no])\n",
    "for i in test_indx[0]:\n",
    "    print(tag_names[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------+----------------+----------+-------------+-------------------+-------------------+\n",
      "|                  Model                  | hyperparameter | Accuracy | HammingLoss | MicroAvg F1-Score | MacroAvg F1-Score |\n",
      "+-----------------------------------------+----------------+----------+-------------+-------------------+-------------------+\n",
      "|    MultinomialNB OneVsRestClassifier    | alpha = 0.001  |  0.0027  |   0.05820   |       0.1092      |       0.0855      |\n",
      "| Logistic Regression OneVsRestClassifier |     C = 1      |  0.1472  |    0.0043   |       0.3242      |       0.1918      |\n",
      "+-----------------------------------------+----------------+----------+-------------+-------------------+-------------------+\n"
     ]
    }
   ],
   "source": [
    "from prettytable import PrettyTable\n",
    "table = PrettyTable()\n",
    "table.field_names = [ \"Model\",\"hyperparameter\", \"Accuracy\",\"HammingLoss\" , \"MicroAvg F1-Score\",\"MacroAvg F1-Score\"]\n",
    "table.add_row([\"MultinomialNB OneVsRestClassifier\",\"alpha = 0.001\", \"0.0027\" , \"0.05820\", \"0.1092\",\"0.0855\"])\n",
    "table.add_row([\"Logistic Regression OneVsRestClassifier\",\"C = 1\",\" 0.1472\",\"0.0043\",\"0.3242\",\"0.1918\"])\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Step by step procedure:</h1>\n",
    "\n",
    "<h2> <font color='blue'>1.  Business Problem: </font></h2>\n",
    "It covers the basic details which should be known before solving the case study.<br>\n",
    "<p>\n",
    "**1.1. Description:** describes the background details of the StackOverFlow website which is must to know to get the insights.<br>\n",
    "**1.2. Problem Statemtent:** describes the problem which we are intended to solve.<br>\n",
    "**1.3. Real World / Business Objectives and Constraints:** describes the objectives which we have to keep in mind while solving the problem. We need to give proper attention towards the constraints stated under this.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> <font color='blue'>2. Machine Learning problem:</font></h2>\n",
    "Looking into the problem as a Machine learning problem.\n",
    "<p>\n",
    "**2.1 Data Overview:** Understanding the data and the data fields.<br>\n",
    "**2.2 Mapping the real-world problem to a Machine Learning Problem:** <br>\n",
    "_2.2.1 Type of Machine Learning Problem:_ Understand the type of problem i.e. classification (binary classification, Multi-class classification, Multi-label classification), regression, etc<br>\n",
    "_2.2.2 Performance Metric:_ Percieve the appropriate metric for this problem.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> <font color='blue'>3. Exploratory Data Analysis:</font></h2>\n",
    "<p>\n",
    "**3.1 Data Loading and Cleaning **<br>\n",
    "*3.1.1 Using Pandas with SQLite to Load the data:* As the size of dataset is too large thus using SQLite for faster implementation.<br>\n",
    "*3.1.2 Counting the number of rows:* understanding data better<br>\n",
    "*3.1.3 Checking for duplicates:* removing any duplicates if present<br>\n",
    "</p>\n",
    "<p>\n",
    "**3.2 Analysis of Tags **<br>\n",
    "*3.2.1 Total number of unique tags: * getting number of unique tags to use them as class labels.<br>\n",
    "*3.2.2 Number of times a tag appeared:* get the frequencies for each tag<br>\n",
    "*3.2.3 Most frequent tag:* keep the most frequent tag and ignore the others for better model.<br>\n",
    "*3.2.4 Top 20 Tags:* to show the top 20 tags in the corpus\n",
    "</p>\n",
    "<p>\n",
    "**3.3 Analysis of Titles**<br>\n",
    "*3.3.1 Similarity between the words in Title and the Tags*\n",
    "</p>\n",
    "<p>\n",
    "**3.4 Preprocessing** <br>\n",
    "<ol>\n",
    "<li> Sample data points</li>\n",
    "<li> Separate out code-snippets from Body </li>\n",
    "<li> Remove Special characters from Question title and description (not in code)</li>\n",
    "<li> Remove stop words (Except 'C')</li>\n",
    "<li> Remove HTML Tags</li>\n",
    "<li> Convert all the characters into small letters</li>\n",
    "<li> Use SnowballStemmer to stem the words</li>\n",
    "</ol>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> <font color='blue'> 4. Machine Learning Models</font></h2>\n",
    "<br>\n",
    "**Converting tags for multilabel problems:** Each tag is to be converted to a label. This allows us to apply Multi-Label Classification problem<br>\n",
    "**4.1 Applying MultinomialNB with OneVsRest Classifier** <br>\n",
    "**4.2. Applying GridSearchCV for Logistic Regression with OneVsRestClassifier**<br>\n",
    "<p> As we are dealing with high dimension data and using OneVsRestClassifier thus Linear models work best.</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
